{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "!pip install pandasql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.special import boxcox1p\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # ignoring annoying warnings\n",
    "from pandasql import sqldf\n",
    "pysqldf = lambda q: sqldf(q, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading_CSV_file\n",
    "features = pd.read_csv(r'C:\\Users\\KINGMAKER\\Downloads\\poova\\Final\\Loan\\Data\\Sales Forecasting\\features.csv')\n",
    "train = pd.read_csv(r'C:\\Users\\KINGMAKER\\Downloads\\poova\\Final\\Loan\\Data\\Sales Forecasting\\train.csv')\n",
    "stores = pd.read_csv(r'C:\\Users\\KINGMAKER\\Downloads\\poova\\Final\\Loan\\Data\\Sales Forecasting\\stores.csv')\n",
    "test = pd.read_csv(r'C:\\Users\\KINGMAKER\\Downloads\\poova\\Final\\Loan\\Data\\Sales Forecasting\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_sto = features.merge(stores, how='inner', on='Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_sto.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shape_of_the_data\n",
    "feat_sto.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datatype\n",
    "pd.DataFrame(feat_sto.dtypes, columns=['Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Type_Train': train.dtypes, 'Type_Test': test.dtypes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date_time_formatting\n",
    "feat_sto.Date = pd.to_datetime(feat_sto.Date)\n",
    "train.Date = pd.to_datetime(train.Date)\n",
    "test.Date = pd.to_datetime(test.Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_sto['Week'] = feat_sto['Date'].dt.isocalendar().week\n",
    "feat_sto['Year'] = feat_sto.Date.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detail = train.merge(feat_sto, \n",
    "                           how='inner',\n",
    "                           on=['Store','Date','IsHoliday']).sort_values(by=['Store',\n",
    "                                                                            'Dept',\n",
    "                                                                            'Date']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_detail = test.merge(feat_sto, \n",
    "                           how='inner',\n",
    "                           on=['Store','Date','IsHoliday']).sort_values(by=['Store',\n",
    "                                                                            'Dept',\n",
    "                                                                            'Date']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del features, train, stores, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns = (train_detail.isnull().sum(axis = 0)/len(train_detail)).sort_values(ascending=False).index\n",
    "null_data = pd.concat([\n",
    "    train_detail.isnull().sum(axis = 0),\n",
    "    (train_detail.isnull().sum(axis = 0)/len(train_detail)).sort_values(ascending=False),\n",
    "    train_detail.loc[:, train_detail.columns.isin(list(null_columns))].dtypes], axis=1)\n",
    "null_data = null_data.rename(columns={0: '# null', \n",
    "                                      1: '% null', \n",
    "                                      2: 'type'}).sort_values(ascending=False, by = '% null')\n",
    "null_data = null_data[null_data[\"# null\"]!=0]\n",
    "null_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pysqldf(\"\"\"\n",
    "SELECT\n",
    "    T.*,\n",
    "    case\n",
    "        when ROW_NUMBER() OVER(partition by Year order by week) = 1 then 'Super Bowl'\n",
    "        when ROW_NUMBER() OVER(partition by Year order by week) = 2 then 'Labor Day'\n",
    "        when ROW_NUMBER() OVER(partition by Year order by week) = 3 then 'Thanksgiving'\n",
    "        when ROW_NUMBER() OVER(partition by Year order by week) = 4 then 'Christmas'\n",
    "    end as Holyday,\n",
    "    case\n",
    "        when ROW_NUMBER() OVER(partition by Year order by week) = 1 then 'Sunday'\n",
    "        when ROW_NUMBER() OVER(partition by Year order by week) = 2 then 'Monday'\n",
    "        when ROW_NUMBER() OVER(partition by Year order by week) = 3 then 'Thursday'\n",
    "        when ROW_NUMBER() OVER(partition by Year order by week) = 4 and Year = 2010 then 'Saturday'\n",
    "        when ROW_NUMBER() OVER(partition by Year order by week) = 4 and Year = 2011 then 'Sunday'\n",
    "        when ROW_NUMBER() OVER(partition by Year order by week) = 4 and Year = 2012 then 'Tuesday'\n",
    "    end as Day\n",
    "    from(\n",
    "        SELECT DISTINCT\n",
    "            Year,\n",
    "            Week,\n",
    "            case \n",
    "                when Date <= '2012-11-01' then 'Train Data' else 'Test Data' \n",
    "            end as Data_type\n",
    "        FROM feat_sto\n",
    "        WHERE IsHoliday = True) as T\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame named train_detail\n",
    "weekly_sales_2010 = train_detail[train_detail['Year'] == 2010]['Weekly_Sales'].groupby(train_detail['Week']).mean()\n",
    "weekly_sales_2011 = train_detail[train_detail['Year'] == 2011]['Weekly_Sales'].groupby(train_detail['Week']).mean()\n",
    "weekly_sales_2012 = train_detail[train_detail['Year'] == 2012]['Weekly_Sales'].groupby(train_detail['Week']).mean()\n",
    "\n",
    "# Now you can fill missing values and convert data types\n",
    "weekly_sales_2010.fillna(0, inplace=True)\n",
    "weekly_sales_2011.fillna(0, inplace=True)\n",
    "weekly_sales_2012.fillna(0, inplace=True)\n",
    "\n",
    "weekly_sales_2010 = weekly_sales_2010.astype(float)\n",
    "weekly_sales_2011 = weekly_sales_2011.astype(float)\n",
    "weekly_sales_2012 = weekly_sales_2012.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_visualization\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.lineplot(weekly_sales_2010.index)\n",
    "sns.lineplot(weekly_sales_2011.index)\n",
    "sns.lineplot(weekly_sales_2012.index)\n",
    "plt.grid()\n",
    "plt.xticks(np.arange(1, 53, step=1))\n",
    "plt.legend(['2010', '2011', '2012'], loc='best', fontsize=16)\n",
    "plt.title('Average Weekly Sales - Per Year', fontsize=18)\n",
    "plt.ylabel('Sales', fontsize=16)\n",
    "plt.xlabel('Week', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "sns.lineplot(weekly_sales_2010.values)\n",
    "sns.lineplot(weekly_sales_2011.values)\n",
    "sns.lineplot(weekly_sales_2012.values)\n",
    "plt.grid()\n",
    "plt.xticks(np.arange(1, 53, step=1))\n",
    "plt.legend(['2010', '2011', '2012'], loc='best', fontsize=16)\n",
    "plt.title('Average Weekly Sales - Per Year', fontsize=18)\n",
    "plt.ylabel('Sales', fontsize=16)\n",
    "plt.xlabel('Week', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detail.loc[(train_detail.Year==2010) & (train_detail.Week==13), 'IsHoliday'] = True\n",
    "train_detail.loc[(train_detail.Year==2011) & (train_detail.Week==16), 'IsHoliday'] = True\n",
    "train_detail.loc[(train_detail.Year==2012) & (train_detail.Week==14), 'IsHoliday'] = True\n",
    "test_detail.loc[(test_detail.Year==2013) & (test_detail.Week==13), 'IsHoliday'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_sales = train_detail['Weekly_Sales'].groupby(train_detail['Store']).mean()\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.barplot(x=weekly_sales.index, y=weekly_sales.values, palette='dark')\n",
    "plt.grid()\n",
    "plt.title('Average Sales - per Store', fontsize=18)\n",
    "plt.ylabel('Sales', fontsize=16)\n",
    "plt.xlabel('Store', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_sales = train_detail['Weekly_Sales'].groupby(train_detail['Dept']).mean()\n",
    "plt.figure(figsize=(25,8))\n",
    "sns.barplot(x=weekly_sales.index, y=weekly_sales.values, palette='dark')\n",
    "plt.grid()\n",
    "plt.title('Average Sales - per Dept', fontsize=18)\n",
    "plt.ylabel('Sales', fontsize=16)\n",
    "plt.xlabel('Dept', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "\n",
    "numeric_columns = train_detail.select_dtypes(include=np.number)\n",
    "corr = numeric_columns.corr()\n",
    "\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool_))\n",
    "\n",
    "f, ax = plt.subplots(figsize=(20, 15))\n",
    "\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "plt.title('Correlation Matrix', fontsize=18)\n",
    "\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detail = train_detail.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'])\n",
    "test_detail = test_detail.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discrete_plot(feature):\n",
    "    fig = plt.figure(figsize=(20,8))\n",
    "    gs = GridSpec(1,2)\n",
    "    sns.boxplot(y=train_detail.Weekly_Sales, x=train_detail[feature], ax=fig.add_subplot(gs[0,0]))\n",
    "    plt.ylabel('Sales', fontsize=16)\n",
    "    plt.xlabel(feature, fontsize=16)\n",
    "    sns.stripplot(y=train_detail.Weekly_Sales, x=train_detail[feature], ax=fig.add_subplot(gs[0,1]))\n",
    "    plt.ylabel('Sales', fontsize=16)\n",
    "    plt.xlabel(feature, fontsize=16)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_continuous_plot(feature):\n",
    "    \n",
    "    fig = plt.figure(figsize=(18,15))\n",
    "    gs = GridSpec(2,2)\n",
    "    \n",
    "    j = sns.scatterplot(y=train_detail['Weekly_Sales'], \n",
    "                        x=boxcox1p(train_detail[feature], 0.15), ax=fig.add_subplot(gs[0,1]), palette = 'blue')\n",
    "\n",
    "    plt.title('BoxCox 0.15\\n' + 'Corr: ' + str(np.round(train_detail['Weekly_Sales'].corr(boxcox1p(train_detail[feature], 0.15)),2)) +\n",
    "              ', Skew: ' + str(np.round(stats.skew(boxcox1p(train_detail[feature], 0.15), nan_policy='omit'),2)))\n",
    "    \n",
    "    j = sns.scatterplot(y=train_detail['Weekly_Sales'], \n",
    "                        x=boxcox1p(train_detail[feature], 0.25), ax=fig.add_subplot(gs[1,0]), palette = 'blue')\n",
    "\n",
    "    plt.title('BoxCox 0.25\\n' + 'Corr: ' + str(np.round(train_detail['Weekly_Sales'].corr(boxcox1p(train_detail[feature], 0.25)),2)) +\n",
    "              ', Skew: ' + str(np.round(stats.skew(boxcox1p(train_detail[feature], 0.25), nan_policy='omit'),2)))\n",
    "    \n",
    "    j = sns.distplot(train_detail[feature], ax=fig.add_subplot(gs[1,1]), color = 'green')\n",
    "\n",
    "    plt.title('Distribution\\n')\n",
    "    \n",
    "    j = sns.scatterplot(y=train_detail['Weekly_Sales'], \n",
    "                        x=train_detail[feature], ax=fig.add_subplot(gs[0,0]), color = 'red')\n",
    "\n",
    "    plt.title('Linear\\n' + 'Corr: ' + str(np.round(train_detail['Weekly_Sales'].corr(train_detail[feature]),2)) + ', Skew: ' + \n",
    "               str(np.round(stats.skew(train_detail[feature], nan_policy='omit'),2)))\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_discrete_plot('IsHoliday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_discrete_plot('Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detail.Type = train_detail.Type.apply(lambda x: 3 if x == 'A' else(2 if x == 'B' else 1))\n",
    "test_detail.Type = test_detail.Type.apply(lambda x: 3 if x == 'A' else(2 if x == 'B' else 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_continuous_plot('Temperature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detail = train_detail.drop(columns=['Temperature'])\n",
    "test_detail = test_detail.drop(columns=['Temperature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_continuous_plot('CPI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detail = train_detail.drop(columns=['CPI'])\n",
    "test_detail = test_detail.drop(columns=['CPI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_continuous_plot('Unemployment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detail = train_detail.drop(columns=['Unemployment'])\n",
    "test_detail = test_detail.drop(columns=['Unemployment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_continuous_plot('Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_selection\n",
    "def WMAE(dataset, real, predicted):\n",
    "    weights = dataset.IsHoliday.apply(lambda x: 5 if x else 1)\n",
    "    return np.round(np.sum(weights*abs(real-predicted))/(np.sum(weights)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random_forest\n",
    "#---------\n",
    "def random_forest(n_estimators, max_depth):\n",
    "    result = []\n",
    "    for estimator in n_estimators:\n",
    "        for depth in max_depth:\n",
    "            wmaes_cv = []\n",
    "            for i in range(1,5):\n",
    "                print('k:', i, ', n_estimators:', estimator, ', max_depth:', depth)\n",
    "                x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n",
    "                RF = RandomForestRegressor(n_estimators=estimator, max_depth=depth)\n",
    "                RF.fit(x_train, y_train)\n",
    "                predicted = RF.predict(x_test)\n",
    "                wmaes_cv.append(WMAE(x_test, y_test, predicted))\n",
    "            print('WMAE:', np.mean(wmaes_cv))\n",
    "            result.append({'Max_Depth': depth, 'Estimators': estimator, 'WMAE': np.mean(wmaes_cv)})\n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_II(n_estimators, max_depth, max_features):\n",
    "    result = []\n",
    "    for feature in max_features:\n",
    "        wmaes_cv = []\n",
    "        for i in range(1,5):\n",
    "            print('k:', i, ', max_features:', feature)\n",
    "            x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n",
    "            RF = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=feature)\n",
    "            RF.fit(x_train, y_train)\n",
    "            predicted = RF.predict(x_test)\n",
    "            wmaes_cv.append(WMAE(x_test, y_test, predicted))\n",
    "        print('WMAE:', np.mean(wmaes_cv))\n",
    "        result.append({'Max_Feature': feature, 'WMAE': np.mean(wmaes_cv)})\n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_III(n_estimators, max_depth, max_features, min_samples_split, min_samples_leaf):\n",
    "    result = []\n",
    "    for split in min_samples_split:\n",
    "        for leaf in min_samples_leaf:\n",
    "            wmaes_cv = []\n",
    "            for i in range(1,5):\n",
    "                print('k:', i, ', min_samples_split:', split, ', min_samples_leaf:', leaf)\n",
    "                x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n",
    "                RF = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features, \n",
    "                                           min_samples_leaf=leaf, min_samples_split=split)\n",
    "                RF.fit(x_train, y_train)\n",
    "                predicted = RF.predict(x_test)\n",
    "                wmaes_cv.append(WMAE(x_test, y_test, predicted))\n",
    "            print('WMAE:', np.mean(wmaes_cv))\n",
    "            result.append({'Min_Samples_Leaf': leaf, 'Min_Samples_Split': split, 'WMAE': np.mean(wmaes_cv)})\n",
    "    return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_detail[['Store','Dept','IsHoliday','Size','Week','Type','Year']]\n",
    "Y_train = train_detail['Weekly_Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [56, 58, 60]\n",
    "max_depth = [25, 27, 30]\n",
    "\n",
    "random_forest(n_estimators, max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = [2, 3, 4, 5, 6, 7]\n",
    "\n",
    "random_forest_II(n_estimators=58, max_depth=30, max_features=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_split = [2, 3, 4]\n",
    "min_samples_leaf = [1, 2, 3]\n",
    "\n",
    "random_forest_III(n_estimators=58, max_depth=30, max_features=7, \n",
    "                  min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestRegressor(n_estimators=58, max_depth=30, max_features=7, min_samples_split=3, min_samples_leaf=1)\n",
    "RF.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_detail[['Store', 'Dept', 'IsHoliday', 'Size', 'Week', 'Type', 'Year']]\n",
    "predict = RF.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final = X_test[['Store', 'Dept', 'Week']]\n",
    "Final['Weekly_Sales'] = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_adj = pysqldf(\"\"\"\n",
    "    SELECT\n",
    "        Store,\n",
    "        Dept,\n",
    "        Week,\n",
    "        Weekly_Sales,\n",
    "        case \n",
    "            when Week = 52 and last_sales > 2*Weekly_Sales then Weekly_Sales+(2.5/7)*last_sales\n",
    "            else Weekly_Sales \n",
    "        end as Weekly_Sales_Adjusted\n",
    "    from(\n",
    "        SELECT\n",
    "            Store, \n",
    "            Dept, \n",
    "            Week, \n",
    "            Weekly_Sales,\n",
    "            case \n",
    "                when Week = 52 then lag(Weekly_Sales) over(partition by Store, Dept) \n",
    "            end as last_sales\n",
    "        from Final)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.DataFrame()\n",
    "sample_submission['Weekly_Sales'] = Final_adj['Weekly_Sales_Adjusted']\n",
    "sample_submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('submission.csv')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
